---
title: "R Notebook"
output: html_notebook
---


# Neural Network
Neural networks are machine learning methods that simulate the biological learning mechanism ofthe brain (Goodfellow et al. (2016), Zhang et al. (2021)). Neural networks allow to learn representation features encompassing complex relations between inputs and output. Representation learning is the use of Machine Learning to not only learn the mapping from representation features tooutput, but also to learn the representation itself. Deep Learning methods introduce representations that are expressed in terms of other simpler representations, thus enabling to derive complex concepts out of simpler concepts. In practice, a neural network is a series of layers, made up of nodes with subsequent layers separated by activation functions.


A number of steps are taken in pre-processing of the data. 
Scaling of numeric variables is important as to not saturate hidden neurons, balancing the learning rate of all neurons.
Categorical variables are one-hot-encoded as to normalise each to a [0,1] range.

The model used is a fully connected deep neural network with 3 layers. As the labels are binary, binary classification is used. ReLU (Rectified Linear Unit) activation is implimented for hidden layers and sigmoid activation on the output layer.
The Adam optimizer with default learning rate of 0.0001 is chosen for it's computaional efficiency and wide adoption in DNN models. 

During tuning, a number of parameters are grid searched through to give results of a number different style models. 

The paramaters used are:

1) Dropout
The first and second layers of neurons have an optional dropout layer. When active the dropout rate is either a low 0.25 of higher value of 0.5. Dropout layers randomly deactivate neurons with a frequency of the specified rate during training as to limit dependence on any subset. 

2) Width Size
This flag equalWidths indicates weither the neural network will have equal widths of neurons at all layers, each the width of the input layer.
If false, each subsiquent layers is half the width of the previous.

3) Regularization
l2 regularization is available on each layer, adding the squared magnitude of coefficients as penalty to the loss function.
This is a feature to help generalise the model and stop overfitting.
Values of 0, 0.001 and 0.0001 are used to tune.

4) Normalization
Batch normalisation is available on each layer. When activated output of each layer is scaled and centered before being passed to subsiquent layers.


The best performing model by validation accuracy has neurons of equal widths, and is utilizing each of regularization, normalization and dropout layers.


```{r}
read_metrics <- function(path, files = NULL){
  path <- paste0(path, "/")
  if ( is.null(files) ) files <- list.files(path)
  n <- length(files)
  out <- vector("list", n)
  for ( i in 1:n ) {
    dir <- paste0(path,'/', files[i], "/tfruns.d/")
    out[[i]] <- jsonlite::fromJSON(paste0(dir, "metrics.json"))
    out[[i]]$flags <- jsonlite::fromJSON(paste0(dir, "flags.json"))
    out[[i]]$evaluation <- jsonlite::fromJSON(paste0(dir, "evaluation.json"))
  }
  return(out)
}

plot_learning_curve <- function(x, ylab = NULL, cols = NULL, top = NULL, topCol = "deepskyblue2", span = 0.4, ...){
  # to add a smooth line to points
  smooth_line <- function(y) {
    x <- 1:length(y)
    out <- predict( loess(y ~ x, span = span) )
    return(out)
  }
  matplot(x, ylab = ylab, xlab = "Epochs", type = "n",...)
  grid()
  matplot(x, pch = 19, col = adjustcolor(cols, 0.3), add = TRUE)
  tmp <- apply(x, 2, smooth_line)
  tmp <- sapply( tmp, "length<-", max(lengths(tmp)) )
  cl <- rep(cols, ncol(tmp))
  cl[top] <- topCol
  matlines(tmp, lty = 1, col = cl, lwd = 2)
}
```

```{r}
# extract results
metrics <- read_metrics(glue(PATH_DB,"NN_tuningRuns_final/"))
# extract validation accuracy
acc <- sapply(metrics, "[[", "val_accuracy")
loss <- sapply(metrics, "[[", "val_loss")
evaluation <- sapply(metrics, "[[", "evaluation")
```


```{r}
val_res = data.frame(
  i = 1:length(metrics),
  val_accuracy = apply(acc, 2, max, na.rm = TRUE),
  val_loss =  apply(loss, 2, min, na.rm = TRUE),
  evaluation = evaluation[2,]
)

val_res = val_res %>% arrange(-val_accuracy)
#val_res = val_res %>% arrange(evaluation)
top3 = val_res[1:3,]

top3
```


Inspecting the accuracy and loss learning curves of the tuning runs there is an evident ceiling / floor that each model reaches.
The best 3 models
Boxplots show a spead of 

```{r}
par(mfrow = c(1,2))
plot_learning_curve(acc, col = adjustcolor("black", 0.3), ylim = c(0.8, 0.9),ylab = "Val accuracy", top = top3$i, topCol = 'skyblue')
plot_learning_curve(loss, col = adjustcolor("black", 0.3), ylim = c(0.2, 0.6),ylab = "Val loss", top = top3$i, topCol = 'orange')
mtext("accuracy and loss training curves", side = 1, line = -22, outer = TRUE)
```

Boxplots
```{r}
par(mfrow = c(1,2))
boxplot(val_res$val_accuracy, outline=FALSE)
boxplot(val_res$val_loss, outline=FALSE)
```

```{r}
#Get best performing model
bestModel = as.data.frame(metrics[[top3[1,]$i]])[1,5:16]

bestModel
```

```{r}
targetPrediction = model_neuralNetwork(getFilePath("data_lastPerCustomerID",".parquet"), tuning = FALSE, bestModelFlags = bestModel)
```


```{r, eval = FALSE}
prediction = ifelse(targetPrediction$prediction>0.5,1,0)[,1]
NNroc = roc(response = targetPrediction$target, predictor = targetPrediction$prediction)
plot(NNroc, print.auc=TRUE)
```


```{r}
par(mfrow = c(1,2))
hist(targetPrediction$target)
hist(ifelse(targetPrediction$prediction>0.5,1,0))
```
