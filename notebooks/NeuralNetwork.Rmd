---
title: "R Notebook"
output: html_notebook
---


# Neural Network
Neural networks are machine learning methods that simulate the biological learning mechanism ofthe brain (Goodfellow et al. (2016), Zhang et al. (2021)). Neural networks allow to learn representation features encompassing complex relations between inputs and output. Representation learning is the use of Machine Learning to not only learn the mapping from representation features tooutput, but also to learn the representation itself. Deep Learning methods introduce representations that are expressed in terms of other simpler representations, thus enabling to derive complex concepts out of simpler concepts. In practice, a neural network is a series of layers, made up of nodes with subsequent layers separated by activation functions.


A number of steps are taken in pre-processing of the data. 
Scaling of numeric variables is important as to not saturate hidden neurons, balancing the learning rate of all neurons.
Categorical variables are one-hot-encoded as to normalise each to a [0,1] range.


The model used is a fully connected deep neural network with 3 layers. As the labels are binary, binary classification is used. ReLU (Rectified Linear Unit) activation is implimented for hidden layers and sigmoid activation on the output layer.
During tuning, a number of parameters are grid searched through to give results of a number different style models. 


The paramaters used are:

1) Dropout
The first and second layers of neurons have an optional dropout layer. When active the dropout rate is either a low 0.25 of higher value of 0.5. Dropout layers randomly deactivate neurons with a frequency of the specified rate during training as to limit dependence on any subset. 

2) Width Size
This flag equalWidths indicates weither the neural network will have equal widths of neurons at all layers, each the width of the input layer.
If false, each subsiquent layers is half the width of the previous.

3) Regularization
l2 regularization is available on each layer, adding the squared magnitude of coefficients as penalty to the loss function.
This is a feature to help generalise the model and stop overfitting.
Values of 0, 0.001 and 0.0001 are used to tune.

4) Normalization
Batch normalisation is available on each layer. When activated output of each layer is scaled and centered before being passed to subsiquent layers.

5) Learning Rate 
Learning Rate 

```{r}

unique(c('B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68','D_63','D_64',getNoiseIntervals()$column))
```




```{r}
 FLAGS <- flags(
     flag_numeric("N_input", ncol(x_train_pca)),
     flag_numeric("dropout", bestModelFlags$flags.dropout),
     flag_boolean("equalWidths", bestModelFlags$flags.equalWidths),
     flag_numeric("lambda", bestModelFlags$flags.lambda),
     flag_boolean("normalization", bestModelFlags$flags.normalization),
     flag_numeric("lr", bestModelFlags$flags.lr),
     flag_numeric("bs", bestModelFlags$flags.bs),
     flag_numeric("epochs", bestModelFlags$flags.epochs),
     flag_numeric("verbose", bestModelFlags$flags.verbose),
     flag_string("activationHidden",bestModelFlags$flags.activationHidden),
     flag_string("activationOut",bestModelFlags$flags.activationOut),
     flag_string("loss",bestModelFlags$flags.loss)
   )
model <- keras_model_sequential() %>%
     layer_dense(units = floor(N_input), input_shape = N_input, activation = FLAGS$activationHidden, name = "layer_1",
                 kernel_regularizer = regularizer_l2(FLAGS$lambda)) %>%
     layer_batch_normalization(center = FLAGS$normalization,scale = FLAGS$normalization) %>%
     layer_dropout(rate = FLAGS$dropout) %>%
     
     layer_dense(units = floor(N_input/ifelse(FLAGS$equalWidths,1,2)), activation = FLAGS$activationHidden, name = "layer_2",
                 kernel_regularizer = regularizer_l2(FLAGS$lambda)) %>%
     layer_batch_normalization(center = FLAGS$normalization,scale = FLAGS$normalization) %>%
     layer_dropout(rate = FLAGS$dropout) %>%
     
     layer_dense(units = floor(N_input/ifelse(FLAGS$equalWidths,1,8)), activation = FLAGS$activationHidden, name = "layer_3",
                 kernel_regularizer = regularizer_l2(FLAGS$lambda)) %>%
     layer_batch_normalization(center = FLAGS$normalization,scale = FLAGS$normalization) %>%
     
     layer_dense(units = 1, activation = FLAGS$activationOut, name = "layer_out") %>%
     compile(loss = FLAGS$loss, 
             metrics = "accuracy",
             optimizer = optimizer_adam(learning_rate = FLAGS$lr ),
     )
model %>% plot_model()
```


```{r}
read_metrics <- function(path, files = NULL){
  path <- paste0(path, "/")
  if ( is.null(files) ) files <- list.files(path)
  n <- length(files)
  out <- vector("list", n)
  for ( i in 1:n ) {
    dir <- paste0(path,'/', files[i], "/tfruns.d/")
    out[[i]] <- jsonlite::fromJSON(paste0(dir, "metrics.json"))
    out[[i]]$flags <- jsonlite::fromJSON(paste0(dir, "flags.json"))
    out[[i]]$evaluation <- jsonlite::fromJSON(paste0(dir, "evaluation.json"))
  }
  return(out)
}

plot_learning_curve <- function(x, ylab = NULL, cols = NULL, top = NULL, topCol = "deepskyblue2", span = 0.4, ...){
  # to add a smooth line to points
  smooth_line <- function(y) {
    x <- 1:length(y)
    out <- predict( loess(y ~ x, span = span) )
    return(out)
  }
  matplot(x, ylab = ylab, xlab = "Epochs", type = "n",...)
  grid()
  matplot(x, pch = 19, col = adjustcolor(cols, 0.3), add = TRUE)
  tmp <- apply(x, 2, smooth_line)
  tmp <- sapply( tmp, "length<-", max(lengths(tmp)) )
  cl <- rep(cols, ncol(tmp))
  cl[top] <- topCol
  matlines(tmp, lty = 1, col = cl, lwd = 2)
}
```

```{r}
# extract results
metrics <- read_metrics(glue(PATH_DB,"NN_tuningRuns_final/"))
# extract validation accuracy
acc <- sapply(metrics, "[[", "val_accuracy")
loss <- sapply(metrics, "[[", "val_loss")
evaluation <- sapply(metrics, "[[", "evaluation")
```


```{r}
val_res = data.frame(
  i = 1:length(metrics),
  val_accuracy = apply(acc, 2, max, na.rm = TRUE),
  val_loss =  apply(loss, 2, min, na.rm = TRUE),
  evaluation = evaluation[2,]
)

val_res = val_res %>% arrange(val_loss)
#val_res = val_res %>% arrange(evaluation)
top3 = val_res[1:3,]

top3
```


Inspecting the accuracy and loss learning curves of the tuning runs there is an evident ceiling / floor that each model reaches.    

```{r}
par(mfrow = c(1,2))
plot_learning_curve(acc, col = adjustcolor("black", 0.3), ylim = c(0.8, 0.9),ylab = "Val accuracy", top = top3$i, topCol = 'skyblue')
plot_learning_curve(loss, col = adjustcolor("black", 0.3), ylim = c(0.2, 0.6),ylab = "Val loss", top = top3$i, topCol = 'orange')
```

Boxplots
```{r}
par(mfrow = c(1,2))
boxplot(val_res$val_accuracy, outline=FALSE)
boxplot(val_res$val_loss, outline=FALSE)
```

The chosen model with lowest loss has a funneled widths shape as opposed to equal widths.   
A lower level of dropout is chosen along with normalization activated, both of which help reduce overfitting.
In contrast regularization is deactivated, thus no l2 penalty is applied.
The best tuned model has the larger batch size with the smallest learning rate, indicating that smaller steps in gradient decent with increased information available has given the best results.


```{r}
#Get best performing model
bestModel = as.data.frame(metrics[[top3[1,]$i]])[1,5:16]

bestModel
```

```{r}
targetPrediction = model_neuralNetwork(getFilePath("data_lastPerCustomerID",".parquet"), tuning = FALSE, bestModelFlags = bestModel)
```


```{r, eval = FALSE}
prediction = ifelse(targetPrediction$prediction>0.5,1,0)[,1]
NNroc = roc(response = targetPrediction$target, predictor = targetPrediction$prediction)
plot(NNroc, print.auc=TRUE)
```


```{r}
par(mfrow = c(1,2))
hist(targetPrediction$target)
hist(ifelse(targetPrediction$prediction>0.5,1,0))
```
