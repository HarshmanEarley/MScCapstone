---
title: "R Notebook"
output: html_notebook
---
```{r}
setwd(PATH_WD) 
```
```{r}
getFilePath("train_labels")
```
```{r}

source(getFilePath("Noise", ".R", checkDBOnly = FALSE))
source(getFilePath("cleansing_v2", ".R", checkDBOnly = FALSE))
```



```{r}
dir(path=glue(PATH_DB,"parquet"),  pattern="cleansed")
```

```{r}
file = 'train_data'
writeCleansedToParquet = function(file, chunkSize = 100000){ 
  file = getFilePath(file,".csv")

  f = function(x,pos){
    x = x %>%  left_join(train_labels, by="customer_ID") %>% removeCleansedCols %>% catagoricalToInts  %>% convertNoiseToInt
    write_parquet(x,  glue(PATH_DB,"parquet/cleansed_",pos))
  }
  
  suppressWarnings(
    read_csv_chunked(
      file,
      SideEffectChunkCallback$new(f), 
      chunk_size = chunkSize,
      progress = TRUE
    )
  )
}

```

```{r}
source(glue(PATH_WD,'config.R'))
writeCleansedToParquet( 'train_data')
```

```{r}
columns = columns[!columns %in% nonNumerics]
```

```{r}
intervals_main

function(){  
  cachePath = glue(PATH_DB,"cache/intervals")
  print(glue("Running intervals_main, saving to ",cachePath))
  
  #read colnames from first row of training data
  columns = (open_dataset(sources = glue(PATH_DB,"parquet"))$schema$names) 
  nonNumerics = c("customer_ID",'S_2', 'B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68')
  columns = columns[!columns %in% nonNumerics]
  
  INTERVALS <<- data.frame(column = character(), interval = numeric(), confidence = numeric())
  
  #Try load from cache
  try({load(cachePath, envir = .GlobalEnv)
    columns = columns[!columns %in%  INTERVALS$column]
  })
  
  cols_N = length(columns)
  
  #Return if we got em all.
  if(cols_N == 0){
    return(1)
  }
  
  #Else calc missing intervals 
  estimateInterval_loadData(columns,cachePath)
}
```


```{r}
open_dataset(sources = glue(PATH_DB,"parquet"))$schema$names
```

```{r}
function(columns,cachePath){
  print("Running estimateInterval_loadData")
  #read in full data from parquet
  DF = readFromParquet(glue(PATH_DB,"parquet"))
  cols_N = length(columns)
  
  #For each column call estimateInterval and save result
  for(i in 1:cols_N){
    print(glue("getting interval for ",columns[i]))
    
    # call estimateInterval for column
    vec = DF[,columns[i]][[1]] %>% na.omit
    res = estimateInterval(vec)
    
    ## Map NA results to NA list
    if(all(is.na(res))){
      res = list(column = columns[i], interval = NA, confidence = NA)
    }
    
    # save to cache
    INTERVALS <<- INTERVALS %>% add_row(column = columns[i], interval = res$interval, confidence = res$confidence)
    save(INTERVALS, file = cachePath)
  }
  
  INTERVALS
}
```

```{r}
columns = c('P_2')
cachePath = glue(PATH_DB,"cache/","interval")
```

```{r}
estimateInterval_loadData

function(columns,cachePath){
  print("Running estimateInterval_loadData")
  #read in full data from parquet
  DF = readFromParquet(glue(PATH_DB,"parquet"))
  cols_N = length(columns)
  
  #For each column call estimateInterval and save result
  for(i in 1:cols_N){
    print(glue("getting interval for ",columns[i]))
    
    # call estimateInterval for column
    vec = DF[,columns[i]][[1]] %>% na.omit
    res = estimateInterval(vec)
    
    ## Map NA results to NA list
    if(all(is.na(res))){
      res = list(column = columns[i], interval = NA, confidence = NA)
    }
    
    # save to cache
    INTERVALS <<- INTERVALS %>% add_row(column = columns[i], interval = res$interval, confidence = res$confidence)
    save(INTERVALS, file = cachePath)
  }
  
  INTERVALS
}function(columns,cachePath){
  print("Running estimateInterval_loadData")
  #read in full data from parquet
  DF = readFromParquet(glue(PATH_DB,"parquet"))
  cols_N = length(columns)
  
  #For each column call estimateInterval and save result
  for(i in 1:cols_N){
    print(glue("getting interval for ",columns[i]))
    
    # call estimateInterval for column
    vec = DF[,columns[i]][[1]] %>% na.omit
    res = estimateInterval(vec)
    
    ## Map NA results to NA list
    if(all(is.na(res))){
      res = list(column = columns[i], interval = NA, confidence = NA)
    }
    
    # save to cache
    INTERVALS <<- INTERVALS %>% add_row(column = columns[i], interval = res$interval, confidence = res$confidence)
    save(INTERVALS, file = cachePath)
  }
  
  INTERVALS
}
```

```{r}
source('C:/Users/sidne/Documents/GitHub/DAC_Project/R/config.R')
```
```{r}
system('pwd')
```


```{r}
gc()
rowsN = 1000000

data = readFromParquet("/Users/root1/Documents/amex-default-prediction/parquet2/")  %>% slice_head(n = rowsN)
#%>%   group_by(customer_ID) %>% filter(S_2 == max(S_2)) %>% ungroup

train_labels = read_csv(getFilePath("train_labels")) %>% slice_head(n = rowsN)
train_labels[,'customer_ID'] =  as.integer(match(train_labels$customer_ID, CUSTOMER_ID))


data_y  = merge(x = data %>% select(customer_ID), y = train_labels, by = "customer_ID", all.x = TRUE)$target
#data = data  %>% dplyr::select(-c(customer_ID,S_2))
data = data  %>% dplyr::select(-customer_ID)

#Scale floats
for(i in 1:ncol(data)){
  if("double" == typeof(data[,i][[1]])){
    data[,i] = scale(data[,i][[1]])
  }
}

#One hot encode catagoricals
catCols = intersect(c('B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_66', 'D_68','D_63','D_64'), colnames(data))
for(i in 1:length(catCols)){
  print(i)
  data = cbind(data, data %>% select(catCols[i]) %>% mutate(across(catCols[i],factor)) %>% as.data.table %>% one_hot()) %>% select(-catCols[i])
}

#Replace all remaining NA with zero
data = data %>% mutate_all(~replace(., is.na(.), 0))
```

```{r}
N = nrow(data)

i_partitions = partition(1:N, p = c(train = 0.7, valid = 0.2, test = 0.1))

# Assign dataframes
x_train = data[i_partitions$train,] %>% as.matrix()  
x_val = data[i_partitions$valid,] %>% as.matrix() 
x_test = data[i_partitions$test,] %>% as.matrix() 
rm(data)

#Assign lables
y_train = data_y[i_partitions$train]
y_val = data_y[i_partitions$valid]
y_test = data_y[i_partitions$test]
rm(data_y)

gc()
```

```{r}
N_input = ncol(x_train)

model <- keras_model_sequential() %>%
  layer_dense(units = floor(N_input/2), activation = 'sigmoid', input_shape = N_input, name = "dense1",
              kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_batch_normalization(center = TRUE,scale = TRUE) %>%
  layer_dropout(rate = 0.01) %>%
  
  layer_dense(units = floor(N_input/4), activation = 'sigmoid', name = "dense2",
                            kernel_regularizer = regularizer_l2(0.01)) %>%
  layer_batch_normalization(center = TRUE,scale = TRUE) %>%
  #layer_dropout(rate = 0.01) %>%
  
  layer_dense(units = floor(N_input/8), activation = 'sigmoid', name = "dense3",
                            kernel_regularizer = regularizer_l2(0.01)) %>%
  #layer_dropout(rate = 0.01) %>%  
  
  layer_dense(units = 1, activation = 'sigmoid', name = "dense4")  %>% 
  compile(loss = 'binary_crossentropy', 
          metrics = "accuracy",
          #optimizer = optimizer_adam(learning_rate = 0.0001)
          optimizer = "adam"
  )

# training and evaluation
fit <- model %>% fit(
  x = x_train, y = y_train,
  validation_data = list(x_val, y_val),
  epochs = 5,
  batch_size = 64,
  verbose = TRUE,
  callbacks = callback_early_stopping(monitor = "val_loss", patience = 5)
)

```

```{r, eval = FALSE}
score <- model %>% evaluate(
  x_test, y_test,
  verbose = 1
)
```

