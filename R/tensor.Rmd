---
title: "R Notebook"
output: html_notebook
---

```{r}
source('cleansing_v2.R')

```

```{r}
writeCleansedCSV = function(file, newFile, chunkSize = 100000){ 
  file = getFilePath(file,".csv")
  newFile = paste(c(newFile,"csv"), collapse = ".")
  newFile = paste(c(strsplit(file, "/")[[1]] %>% head(-1),newFile), collapse = "/") #put new file in same dir as old
  
  f = function(x,pos){
    x = x %>%  left_join(train_labels, by="customer_ID") %>% removeCleansedCols %>% removeNonNumerics
    write_csv(x, newFile, append = ifelse(pos == 1, FALSE, TRUE))
  }
  
  suppressWarnings(
    read_csv_chunked(
      file,
      SideEffectChunkCallback$new(f), 
      chunk_size = chunkSize,
      progress = TRUE
    )
  )
}
```
```{r}
head(asd)
```

```{r}

traindata = read_csv(getFilePath("train_data_head"))
```


```{r}
dim(traindata)
dim(headval)
```

```{r}
headval = read_csv(getFilePath("train_data"), n_max = 300000, skip = 400000)

colnames(headval) = colnames(traindata)

colnames(headval)
```

```{r}
write_csv(headval, "/Users/root1/Documents/amex-default-prediction//csv/val_data_head.csv")
```

```{r}
read_csv(getFilePath("val_data_head"), n_max = 300)
```


```{r}
writeCleansedCSV("val_data_head", "val_data_head_clean", chunkSize = 100000)
```


```{r}
read_csv(getFilePath("val_data_head_clean"), n_max = 300)
```





```{r, echo=FALSE}
library(tensorflow)
tensorflow::install_tensorflow(restart_session = FALSE)
library(keras)
install_keras(restart_session = FALSE)
```


```{r}
#asd = read_csv(getFilePath("train_data_head_clean"))

colSums(is.na(asd))
```

```{r}
length(trainSpec$names[!trainSpec$names %in% c("customer_ID","target")])
```


```{r}
asd$B_31
```

```{r}
length(
  (trainSpec$names)[!trainSpec$names %in% c("customer_ID","target","B_31")]
)
```

```{r}
 iterator_get_next(iter)
```

```{r}

sess <- tf$compat$v1$Session()


trainSpec = csv_record_spec(getFilePath("train_data_head_clean"))
dataset <- text_line_dataset(getFilePath("train_data_head_clean"), record_spec = trainSpec, parallel_records = 4) %>%
  dataset_prepare(x = -c(customer_ID,target,B_31), y = target, batch = 128) %>% 
  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10)  %>% 
  dataset_prefetch_to_device("/gpu:0")

model <- keras_model_sequential() %>%
  layer_dense(units = 27, activation = 'relu', input_shape = c(54)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 13, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, activation = 'sigmoid'
)

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

iter <- make_iterator_one_shot(dataset)

until_out_of_range({
  next_batch <- iterator_get_next(iter)
  history <- model %>% train_on_batch(next_batch$x, next_batch$y)
})

```

```{r}
model
```


```{r}
data,lables = iterator_get_next(iter)
```

```{r}
next_batch$x
```

```{r}
tf$compat$v1$Session()
```
```{r}
getDataset <- function(filename){
  filename = getFilePath(filename)
  dataset <- text_line_dataset(filename, record_spec = trainSpec, parallel_records = 4) %>%
  dataset_prepare(x = -c(customer_ID,target,B_31), y = target, batch = 128) %>% 
  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10)  %>% 
  dataset_prefetch_to_device("/gpu:0")
  
  dataset
}


dataset = getDataset("train_data_head_clean")
```


```{r}
getDataset <- function(filename) {
  dataset <- text_line_dataset(filename, record_spec = trainSpec, parallel_records = 4) %>%
  dataset_prepare(x = -c(customer_ID,target,B_31), y = target, batch = 128) %>% 
  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10)  %>% 
  dataset_prefetch_to_device("/gpu:0")
}
```

```{r}
dataset
```

```{r}
getDataset <- function(filename) {
  filename = getFilePath("train_data_head_clean")
  dataset <- text_line_dataset(filename, record_spec = trainSpec, parallel_records = 4) %>%
  dataset_prepare(x = -c(customer_ID,target,B_31), y = target, batch = 128) %>% 
  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10)  %>% 
  dataset_prefetch_to_device("/gpu:0")
  
  dataset
}

getDataset()
```

```{r}

batch_size = 128
steps_per_epoch = 500

# function to read and preprocess mnist dataset
getDataset <- function(filename) {
  filename = getFilePath(filename)
  dataset <- text_line_dataset(filename, record_spec = trainSpec, parallel_records = 4) %>%
  dataset_prepare(x = -c(customer_ID,target,B_31), y = target, batch = 128) %>% 
  dataset_shuffle_and_repeat(buffer_size = 1000, count = 10)  %>% 
  dataset_prefetch_to_device("/gpu:0")
  
  dataset
}

model <- keras_model_sequential() %>%
  layer_dense(units = 27, activation = 'relu', input_shape = c(54)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 13, activation = 'relu') %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, activation = 'sigmoid'
)

model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)

history <- model %>% fit(
  getDataset("train_data_head_clean"),
  steps_per_epoch = steps_per_epoch,
  epochs = 20
  #validation_data = getDataset("val_data_head_clean"),
  #validation_steps = steps_per_epoch,
  #shuffle = TRUE
)

#score <- model %>% evaluate(
#  mnist_dataset("mnist/test.tfrecords"),
#  steps = steps_per_epoch
#)

#print(score)
```


```{r}
getDataset("train_data_head_clean")
```

```{r}
fit_generator
```

