# Methods

## Random Forest (RF)

Random Forests, \citet{rfor}, are a very powerful ensemble classification method. Ensemble learning is an aggregation of predictions made by multiple classifiers with the goal of improving accuracy. The method uses Classification Trees and bootstrapping extensively. A random forest is, effectively, a random collection of Classification Trees estimated on random subsets of the data.  

A Classification Tree is an iterative process of splitting the data into partitions based on values of the observations, and then splitting it up further on each of the branches. The classifier is trained in order to produce pure groups of observations or 'buckets', by minimising the entropy or spread of the target variable in each bucket. The majority value of the target variable in a bucket is then used for predictions. The main disadvantage is that Classification Trees suffer from over-fitting and bias and using a single Classification Tree would present an over simplistic model.  

In a Random Forest, each Classification Tree is  grown independently, and each individual Classification Tree has an equal vote as to what the outcome is. Random forests provide an improvement by means of a small tweak that reduces the dependence among the trees. The main idea is to use only a random subset of the predictor variables at each split of the classification tree fitting step, this is generally taken as $\sqrt{N}$ where N is the number of features but can be specified by the user. If we build classifiers on subsets of the variables, then they will behave more independently than if we build them on all of the data. This increases diversity and averaging results across independent classifiers will be more stable than averaging results on dependent ones. The main parameter of the Random Forests to be tuned is the number of Classification Trees used and is tuned to prevent overfitting and improve efficiency.  

## Support Vector Machines (SVM)

Support Vector Machines, \citet{svm1} and \citet{svm2}, is a classification method based on projecting the data into a higher dimensional space where distinction between the target variable groups is clearer. With regards to a binary classification problem, data is linearly separable if there exists a separating hyperplane in the input space of the training data that fully separates observations by the value of the target variable. This is known as a linear classifier. In reality data can be quite messy and such hyperplanes are not likely to exist for a specific problem, especially in higher dimension problems.  

Support Vector Machines use a kernel function to map the training data of the input space into a high-dimensional feature space. Scenarios that are inseparable in the original space can be linearly classified in the high-dimensional space. A kernel function $K()$ is a generalization of the inner product of the form: $K(x_i, x_h) = \phi(x_i)^T \phi (x_h)$ where $\phi()$ is some mapping function of the data and $x_i$,$x_i$ are generic input observations. To avoid the explicit mapping to the higher-dimensional space $Q$ which could be computationally inefficient, the kernel computes the inner products via the kernel in the original input feature space $\mathbb{R}^V$. The kernel function returns the inner product between two points in the enlarged feature space with little computational cost even in very high-dimensional spaces. Kernelizing methods can help to account for non-linear patterns and separate groups and classes for clustering or classification.   

The predictive performance of SVM can be very sensitive to the choice of the kernel and the cost, which is a measure of tolerance a function has to observations violating hyperplane and its margins.A classifier with a large cost will strive to getting all the (training) data points classified correctly, conversely, a classifier with a small cost will be tolerant to a certain degree of misclassified observations.

Types of kernels + chosen

## Deep Neural Network (DNN)

Neural networks are machine learning methods that simulate the biological learning mechanism of the brain (\citet{Goodfellow-et-al-2016}, \citet{zhang2021dive}). Neural networks allow to learn representation features encompassing complex relations between inputs and output. Representation learning is the use of Machine Learning to not only learn the mapping from representation features to output, but also to learn the representation itself. Deep Learning methods introduce representations that are expressed in terms of other simpler representations, thus enabling to derive complex concepts out of simpler concepts.

In practice, a neural network is a series of layers, made up of nodes with subsequent layers separated by activation functions.

## Evaluation








