@article{dnn,
author = {Hassan, Malik and Mirza, Tabasum},
year = {2020},
month = {07},
pages = {},
title = {Credit Card Default Prediction Using Artificial Neural Networks}
}

@inproceedings{Neema2017TheCO,
  title={The comparison of machine learning methods to achieve most cost-effective prediction for credit card default},
  author={Shantanu Neema and Benjamin Soibam},
  year={2017}
}

@INPROCEEDINGS{8776802,
  author={Sayjadah, Yashna and Hashem, Ibrahim Abaker Targio and Alotaibi, Faiz and Kasmiran, Khairl Azhar},
  title={Credit Card Default Prediction using Machine Learning Techniques}, 
  year={2018},
  volume={},
  number={},
  pages={1-4},
  doi={10.1109/ICACCAF.2018.8776802}}

@INPROCEEDINGS{8682212,
  author={Hsu, Te-Cheng and Liou, Shing-Tzuo and Wang, Yun-Ping and Huang, Yung-Shun and Che-Lin},
  booktitle={ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Enhanced Recurrent Neural Network for Combining Static and Dynamic Features for Credit Card Default Prediction}, 
  year={2019},
  volume={},
  number={},
  pages={1572-1576},
  doi={10.1109/ICASSP.2019.8682212}}


@ARTICLE{Yang2018-rt,
  title     = "Comparison of several data mining methods in credit card default
               prediction",
  author    = "Yang, Shenghui and Zhang, Haomin",
  abstract  = "LightGBM is an open-source, distributed and high-performance GB
               framework built by Microsoft company. LightGBM has some
               advantages such as fast learning speed, high parallelism
               efficiency and high-volume data, and so on. Based on the open
               data set of credit card in Taiwan, five data mining methods,
               Logistic regression, SVM, neural network, Xgboost and LightGBM,
               are compared in this paper. The results show that the AUC,
               F1-Score and the predictive correct ratio of LightGBM are the
               best, and that of Xgboost is second. It indicates that LightGBM
               or Xgboost has a good performance in the prediction of
               categorical response variables and has a good application value
               in the big data era.",
  journal   = "Intell. Inf. Manag.",
  publisher = "Scientific Research Publishing, Inc,",
  volume    =  10,
  number    =  05,
  pages     = "115--122",
  year      =  2018
}

@article{YEH20092473,
title = {The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients},
journal = {Expert Systems with Applications},
volume = {36},
number = {2, Part 1},
pages = {2473-2480},
year = {2009},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2007.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0957417407006719},
author = {I-Cheng Yeh and Che-hui Lien},
keywords = {Banking, Neural network, Probability, Data mining},
abstract = {This research aimed at the case of customers’ default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel “Sorting Smoothing Method” to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y=A+BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.}
}

@article{FITZPATRICK2016427,
title = {An empirical comparison of classification algorithms for mortgage default prediction: evidence from a distressed mortgage market},
journal = {European Journal of Operational Research},
volume = {249},
number = {2},
pages = {427-439},
year = {2016},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2015.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S0377221715008383},
author = {Trevor Fitzpatrick and Christophe Mues},
keywords = {Boosting, Random forests, Semi-parametric models, Mortgages, Credit scoring},
abstract = {This paper evaluates the performance of a number of modelling approaches for future mortgage default status. Boosted regression trees, random forests, penalised linear and semi-parametric logistic regression models are applied to four portfolios of over 300,000 Irish owner-occupier mortgages. The main findings are that the selected approaches have varying degrees of predictive power and that boosted regression trees significantly outperform logistic regression. This suggests that boosted regression trees can be a useful addition to the current toolkit for mortgage credit risk assessment by banks and regulators.}
}

@mastersthesis{trap5146,
           month = {September},
           title = {Improving Credit Default Prediction Using Explainable AI},
          school = {Dublin, National College of Ireland},
          author = {Ciaran Egan},
            year = {2021},
             url = {http://norma.ncirl.ie/5146/},
        abstract = {Despite recent improvements in machine-learning prediction methods, the methods used by most lenders to predict credit defaults have not changed. This is because most of the high-performing methods are of a black-box nature. It is a requirement that credit default prediction models be explainable. This research creates credit default prediction models using tree-based ensemble methods. It is shown that model performance can be improved by using gradient boosting methods over traditional credit default predictions models. The top performing XGBoost model is then taken and made explainable. This research proposes a model-agnostic counterfactual extraction algorithm that explains the drivers behind a particular prediction. The algorithm focuses on extracting the counterfactuals that have the fewest contrasting features. This results in counterfactuals that are easily understood by humans and can be easily translated into insights that the lay user can understand. A definite standard of explainability is defined and the counterfactual extraction algorithm results in explanations that meet this standard. Given that the explanation method is model agnostic, it can be used on any prediction model and can be deployed for a wide range of applications.}
}


@article{dnn2,
author = {Chou, Tsungnan and Lo, Mingmin},
year = {2018},
month = {01},
pages = {105-110},
title = {Predicting Credit Card Defaults with Deep Learning and Other Machine Learning Models},
volume = {10},
journal = {International Journal of Computer Theory and Engineering},
doi = {10.7763/IJCTE.2018.V10.1208}
}


@article{rfor,
author = {Breiman, Leo},
title = {Random Forests},
year = {2001},
issue_date = {October 1 2001},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {45},
number = {1},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1010933404324},
doi = {10.1023/A:1010933404324},
abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
journal = {Mach. Learn.},
month = {oct},
pages = {5–32},
numpages = {28},
keywords = {classification, regression, ensemble}
}

@INPROCEEDINGS{svm1,
    author = {Bernhard E. Boser and Isabelle M. Guyon and Vladimir N. Vapnik},
    title = {A Training Algorithm for Optimal Margin Classifiers},
    booktitle = {Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory},
    year = {1992},
    pages = {144--152},
    publisher = {ACM Press}
}

@article{svm2,
author = {Cortes, Corinna and Vapnik, Vladimir},
title = {Support-Vector Networks},
year = {1995},
issue_date = {Sept. 1995},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {20},
number = {3},
issn = {0885-6125},
url = {https://doi.org/10.1023/A:1022627411411},
doi = {10.1023/A:1022627411411},
abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.},
journal = {Mach. Learn.},
month = {sep},
pages = {273–297},
numpages = {25},
keywords = {neural networks, pattern recognition, efficient learning algorithms, radial basis function classifiers, polynomial classifiers}
}


@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


@article{zhang2021dive,
    title={Dive into Deep Learning},
    author={Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
    journal={arXiv preprint arXiv:2106.11342},
    year={2021}
}


@book{alma9945667093902959,
author = {Harrell, Frank E.},
address = {Cham},
booktitle = {Regression modeling strategies : with applications to linear models, logistic and ordinal regression, and survival analysis},
edition = {Second edition.},
isbn = {9783319194257},
keywords = {Regression analysis.},
publisher = {Springer},
series = {Springer series in statistics},
title = {Regression modeling strategies : with applications to linear models, logistic and ordinal regression, and survival analysis  / Frank E. Harrell, Jr.},
year = {2015},
language = {eng},
}

@article{JSSv095i10,
 title={Computing the Kolmogorov-Smirnov Distribution When the Underlying CDF is Purely Discrete, Mixed, or Continuous},
 volume={95},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v095i10},
 doi={10.18637/jss.v095.i10},
 abstract={The distribution of the Kolmogorov-Smirnov (KS) test statistic has been widely studied under the assumption that the underlying theoretical cumulative distribution function (CDF), F (x), is continuous. However, there are many real-life applications in which fitting discrete or mixed distributions is required. Nevertheless, due to inherent difficulties, the distribution of the KS statistic when F (x) has jump discontinuities has been studied to a much lesser extent and no exact and efficient computational methods have been proposed in the literature. In this paper, we provide a fast and accurate method to compute the (complementary) CDF of the KS statistic when F (x) is discontinuous, and thus obtain exact p values of the KS test. Our approach is to express the complementary CDF through the rectangle probability for uniform order statistics, and to compute it using fast Fourier transform (FFT). Secondly, we provide a C++ and an R implementation of the proposed method, which fills the existing gap in statistical software. We give also a useful extension of the Schmid’s asymptotic formula for the distribution of the KS statistic, relaxing his requirement for F (x) to be increasing between jumps and thus allowing for any general mixed or purely discrete F (x). The numerical performance of the proposed FFT-based method, implemented both in C++ and in the R package KSgeneral, available from https://CRAN.R-project.org/package=KSgeneral, is illustrated when F (x) is mixed, purely discrete, and continuous. The performance of the general asymptotic formula is also studied.},
 number={10},
 journal={Journal of Statistical Software},
 author={Dimitrova, Dimitrina S. and Kaishev, Vladimir K. and Tan, Senren},
 year={2020},
 pages={1–42}
}
